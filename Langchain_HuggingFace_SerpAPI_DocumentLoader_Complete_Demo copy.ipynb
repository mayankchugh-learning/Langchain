{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GJa74fymEAc"
   },
   "source": [
    "## Lanchain Docs: https://python.langchain.com/docs/get_started/introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOVhfcWdmHvf"
   },
   "source": [
    "## Install Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SCe1tyZZjPTR",
    "outputId": "c1d1b375-59c3-4a66-bb35-bf84754fa2ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.1.11-py3-none-any.whl (807 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.25 (from langchain)\n",
      "  Downloading langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-core<0.2,>=0.1.29 (from langchain)\n",
      "  Downloading langchain_core-0.1.30-py3-none-any.whl (256 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.23-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.29->langchain) (3.7.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.29->langchain) (23.2)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.2.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: orjson, mypy-extensions, marshmallow, jsonpointer, typing-inspect, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.11 langchain-community-0.0.27 langchain-core-0.1.30 langchain-text-splitters-0.0.1 langsmith-0.1.23 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.9.15 typing-inspect-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "betskPwRmUFK"
   },
   "source": [
    "## Setup environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "mEtnQhXymLtL"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "SERPAPI_API_KEY = userdata.get('SERPAPI_API_KEY')\n",
    "HUGGINGFACEHUB_API_TOKEN = userdata.get('HUGGINGFACE_API_KEY')\n",
    "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da8XlSqDmbol"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "os.environ[\"HUGGINGFACE_API_KEY\"] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf45wnJAmkq0"
   },
   "source": [
    "## Open AI\n",
    "\n",
    "Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0t8M7uCrmeQR",
    "outputId": "e4c6f6e4-69d2-415c-8c19-a597795a70fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Installing collected packages: h11, httpcore, httpx, openai\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.13.3\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQHOhiYNnJvm"
   },
   "source": [
    "In the context of the OpenAI language model, when the temperature parameter is not explicitly specified, it typically defaults to a value of 1.0.\n",
    "\n",
    "Setting the temperature to 1.0 means that the model's output is relatively balanced in terms of randomness and conservatism. It produces responses that are diverse and creative while still maintaining coherence and relevance to the input. This is often a commonly used temperature value when generating text with the OpenAI language model.\n",
    "\n",
    "However, it's important to note that different versions or implementations of the OpenAI language model may have different default temperature values, so it's always a good practice to consult the specific documentation or code reference for the particular version being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CyPuppOoh-B"
   },
   "source": [
    "temperature value--> how creative we want our model to be\n",
    "\n",
    "0 ---> temperature it means model is  very safe it is not taking any bets.\n",
    "\n",
    "1 --> it will take risk it might generate wrong output but it is very creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZOCNsBYmnez"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9sOHjjumuR2"
   },
   "outputs": [],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0nNxzgwwolLa",
    "outputId": "6e29f415-959c-43cc-dca3-df5bf1b3ae4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Rainbow Sox Co.\" \n"
     ]
    }
   ],
   "source": [
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "snFlGm-KoopN",
    "outputId": "f0127deb-7481-40f7-dfe7-270005050e01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Rainbow Socks Co.\" or \"Vibrant Socks Co.\"\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzvfInIRppEp",
    "outputId": "24676ef1-ee59-4b78-eeb4-4041c1b2f146"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Rainbow Threads\" or \"Vibrant Soles\"\n"
     ]
    }
   ],
   "source": [
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVPkqmVUqCWv"
   },
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvjEB1uppr8R"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iX0NO1rqEuf"
   },
   "outputs": [],
   "source": [
    "text = \"I want to open a restaurant for Italian, Indian, and Chinese food. Suggest a fency name for this.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4au6eR_9qPJ6",
    "outputId": "a9f751ec-ce04-4167-98f7-31a2db170d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Palazzo Fusion\"\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJSDnKeorEXO"
   },
   "source": [
    "## HuggingFace API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jQGV17FqRIZ",
    "outputId": "f01f3a33-350b-47ac-a190-6305c208a61c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XucT_9gUrMip"
   },
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qhYpR68Ureqk"
   },
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(repo_id = \"google/flan-t5-large\", model_kwargs={\"temperature\":0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "3Nj9KrpartOD",
    "outputId": "683c4016-6afa-4359-e6e3-7a3fd58ffd77"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Wie sind Sie?'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Translate English to German: How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OKR0D11esPW5",
    "outputId": "19ab8737-0030-4ed8-af38-d3b1e0f84715"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' ?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Translate English to Hindi: How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SfeNId7-sy6B",
    "outputId": "f92c4374-fbbb-4664-c589-00b23440592e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Italian\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceHub(repo_id = \"google/flan-t5-large\", model_kwargs={\"temperature\":0, \"max_length\":64})\n",
    "name = llm.predict(\"I want to open a restaurant for Italian, Indian, and Chinese food. Suggest a fency name for this.\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "4ob4y4AZtH-s",
    "outputId": "5dbcf837-0a04-40ee-a4e9-a191e38d5a52"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Translate english to hindi : How are you?\\n\\nTranslation of How are you in Hindi: आप कैसे हैं?\\n\\nAlso Read: How to say I love you in Hindi\\n\\n## How to say How are you in Hindi\\n\\nTo say How are you in Hindi, you can say आप कैसे हैं?\\n\\nThis is the most common way to ask someone how they are doing in Hindi.\\n\\nHere'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm= HuggingFaceHub(repo_id='mistralai/Mixtral-8x7B-Instruct-v0.1',model_kwargs={'temperature':1})\n",
    "\n",
    "llm(\"Translate english to hindi : How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flguxI9yt2u2"
   },
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guv2Xcu9t6Eg"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "llm = OpenAI(temperature= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LdtDyO6xujTP",
    "outputId": "ffb80f8a-0594-4846-e688-966d7aed9311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to open a restaurant for Bengali food. Suggest a fency name for this.\n"
     ]
    }
   ],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "p = prompt_template_name.format(cuisine=\"Bengali\")\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ZuLqPUROu-IE",
    "outputId": "7c0f7af8-60ec-4c07-e8a5-819ba600f471"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'What is a good name for a company that makes colorful socks'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oW4zQUgFwK-P"
   },
   "source": [
    "# Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM-OR24NxBF5"
   },
   "source": [
    "Central to LangChain is a vital component known as LangChain Chains, forming the core connection among one or several large language models (LLMs). In certain sophisticated applications, it becomes necessary to chain LLMs together, either with each other or with other elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FzyY21jws6P"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "llm = OpenAI(temperature= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-kPhnChdvzYY",
    "outputId": "80511332-12b0-41b9-d628-d584d46556c7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'What is a good name for a company that makes colorful socks'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BhQ_WpCHwglI",
    "outputId": "f958e895-e8b3-44d3-e1c8-5a1589ec8bc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Chroma Sox\n",
      "2. Rainbow Threads\n",
      "3. Technicolor Toes\n",
      "4. Kaleidosock\n",
      "5. Vibrant Verve\n",
      "6. Colorful Kicks Co.\n",
      "7. Bright Feet\n",
      "8. Funky Footwear Inc.\n",
      "9. Hue & Comfort\n",
      "10. Sock Spectrum\n",
      "11. Dapper Dots\n",
      "12. Patterned Peds\n",
      "13. Bold Strides\n",
      "14. Quirky Quads\n",
      "15. Socktopia\n",
      "16. Snazzy Socks Co.\n",
      "17. Pop of Color\n",
      "18. Crazy for Colors\n",
      "19. Sock Symphony\n",
      "20. Happy Feet Co.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "response= chain.run(\"colorful socks\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLCtjlzfwjbA",
    "outputId": "63362b3b-6a3d-4967-d61d-e9adef4c7e2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is a good name for a company that makes colorful socks\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Rainbow Feet Co.\n",
      "Vivid Socks Inc.\n",
      "Color Splash Socks\n",
      "Chroma Socks Co.\n",
      "Kaleidosock Company\n",
      "BrightStep Socks\n",
      "Hueful Hosiery\n",
      "Spectrum Soles\n",
      "Prism Peds\n",
      "Painted Toes Socks LLC\n",
      "Chromatic Creations Co.\n",
      "Funky Footwear Inc.\n",
      "Vibrant Vines Socks\n",
      "Dye Hard Socks Co.\n",
      "Sole Spectrum Co.\n",
      "BoldSteps Inc.\n",
      "Spectrum Socks Co.\n",
      "ColorCraze Co.\n",
      "Rainbow Threads Inc.\n",
      "Vibrant Ventures Co.\n",
      "Sock Spectrum Co.\n",
      "Pigment Peddler Co. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, verbose = True)\n",
    "response= chain.run(\"colorful socks\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sTk8V2R8NYQ"
   },
   "source": [
    "## Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOCWRPXyyPaX"
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.6)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",
    ")\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cEYa8aB8RaD"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N7QGq3lt8T8i",
    "outputId": "63eebc62-a0c1-41c5-b890-35d9537b934c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Tandoori Chicken: Marinated chicken cooked in a clay oven, served with rice and naan bread.\n",
      "2. Butter Chicken: Succulent pieces of chicken simmered in a creamy tomato and butter sauce.\n",
      "3. Lamb Vindaloo: A spicy and tangy curry with tender pieces of lamb.\n",
      "4. Vegetable Korma: A medley of seasonal vegetables cooked in a creamy and aromatic sauce.\n",
      "5. Samosas: Crispy pastries filled with spiced potatoes and peas.\n",
      "6. Chicken Tikka Masala: Grilled chicken pieces in a rich and flavorful tomato-based sauce.\n",
      "7. Palak Paneer: Fresh spinach and cottage cheese cubes cooked in a creamy sauce.\n",
      "8. Biryani: Fragrant basmati rice cooked with spices, vegetables, and your choice of chicken, lamb, or shrimp.\n",
      "9. Garlic Naan: Traditional Indian flatbread infused with garlic and cooked in a clay oven.\n",
      "10. Mango Lassi: A refreshing yogurt drink made with mango puree and spices.\n"
     ]
    }
   ],
   "source": [
    "content = chain.run(\"indian\")\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pg60O3DN8Zpp"
   },
   "source": [
    "# Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x3MJR4VD8WAN"
   },
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")\n",
    "\n",
    "name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")\n",
    "\n",
    "\n",
    "\n",
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables = ['restaurant_name'],\n",
    "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
    ")\n",
    "\n",
    "food_items_chain =LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7DpLVfH8da0"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains = [name_chain, food_items_chain],\n",
    "    input_variables = ['cuisine'],\n",
    "    output_variables = ['restaurant_name', \"menu_items\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SEqeWCGt8gJC",
    "outputId": "984cf78d-b3e9-485c-95fe-29c1c57ff900"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuisine': 'indian', 'restaurant_name': '\\n\\n\"Saffron Spice Kitchen\"', 'menu_items': '\\n\\n1. Saffron Chicken Tikka Masala\\n2. Lamb Kebabs with Saffron Rice\\n3. Saffron Shrimp Curry\\n4. Vegetable Biryani with Saffron Basmati Rice\\n5. Saffron-infused Lentil Soup\\n6. Saffron and Pistachio Crusted Salmon\\n7. Saffron and Raisin Pilaf\\n8. Saffron and Cardamom Flavored Kheer (Rice Pudding)\\n9. Saffron and Almond Chicken Korma\\n10. Saffron and Cumin Spiced Lamb Chops\\n11. Saffron and Spinach Stuffed Naan Bread\\n12. Saffron and Cashew Chicken Curry\\n13. Saffron and Date Chutney\\n14. Saffron and Yogurt Marinated Tandoori Chicken\\n15. Saffron and Coconut Milk Rice Pudding.'}\n"
     ]
    }
   ],
   "source": [
    "print(chain({\"cuisine\": \"indian\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stZPRe7rBrR6"
   },
   "source": [
    "## Agents and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_6mZ6Rc8iCL",
    "outputId": "fdf0aaee-f5ad-44e0-e54a-b20974838ca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2024.2.2)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32003 sha256=e3127cc907744f4f129d9aeda2ee55f09caf0336fd49aefa21b324b439a69cfd\n",
      "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwX9P8akBxr_"
   },
   "source": [
    "https://serpapi.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "zc2N1rY19HWt"
   },
   "outputs": [],
   "source": [
    "os.environ['SERPAPI_API_KEY'] = SERPAPI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "XxHWwdeFB0Yv",
    "outputId": "4201f075-6580-4cbd-d75a-a24212e1b0b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should search for the answer using a search engine.\n",
      "Action: Search\n",
      "Action Input: \"GDP of US in 2023\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m$27.36 trillion\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should use a calculator to convert the GDP from dollars to another currency if needed.\n",
      "Action: Calculator\n",
      "Action Input: $27.36 trillion\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 27360000000000\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The GDP of US in 2023 was $27.36 trillion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The GDP of US in 2023 was $27.36 trillion.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "#Google Search API\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"What was the GDP of US in 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jhk2DmjJB2Fk",
    "outputId": "115b74f6-97ec-4d32-8d31-78d33b87ee15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=d66f019c3fcd2ff72230c3732621ffa02fc036c361efcd1389670f5c7af774e3\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "VQkf3BWpB7Rp",
    "outputId": "ae7fef14-6ca5-46d0-9b95-5f9954771f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the release year of the film and then use the Calculator to raise it to the 0.43 power.\n",
      "Action: wikipedia\n",
      "Action Input: Departed film\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.10/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: The Departed\n",
      "Summary: The Departed is a 2006 American crime thriller film directed by Martin Scorsese and written by William Monahan. It is both a remake of the 2002 Hong Kong film Infernal Affairs and also loosely based on the real-life Boston Winter Hill Gang; the character Colin Sullivan is based on the corrupt FBI agent John Connolly, while the character Frank Costello is based on Irish-American gangster and crime boss Whitey Bulger. The film stars Leonardo DiCaprio, Matt Damon, Jack Nicholson, and Mark Wahlberg, with Martin Sheen, Ray Winstone, Vera Farmiga, Alec Baldwin, Anthony Anderson and James Badge Dale in supporting roles.\n",
      "The film takes place in Boston and the surrounding metro area, primarily in the South Boston neighborhood. Irish Mob boss Frank Costello (Nicholson) plants Colin Sullivan (Damon) as a spy within the Massachusetts State Police; simultaneously, the police assign undercover state trooper Billy Costigan (DiCaprio) to infiltrate Costello's mob crew. When both sides realize the situation, Sullivan and Costigan each attempt to discover the other's identity before they are found out.\n",
      "The Departed was a critical and commercial success, receiving acclaim for its direction, performances (particularly of DiCaprio, Nicholson, and Wahlberg), screenplay, and editing.\n",
      "It won several accolades, including four Oscars at the 79th Academy Awards: for Best Picture, Best Director, Best Adapted Screenplay, and Best Film Editing. It remains Scorsese's only personal Oscar win. The film also received six nominations each at the 64th Golden Globe Awards and the 60th British Academy Film Awards, and two nominations at the 13th Screen Actors Guild Awards.\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Now that I know the release year, I can use the Calculator to raise it to the 0.43 power.\n",
      "Action: Calculator\n",
      "Action Input: 2006 ^ 0.43\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 26.30281917656938\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: 26.30281917656938\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'26.30281917656938'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# install this package: pip install wikipedia\n",
    "\n",
    "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\"In what year was the film Departed with Leopnardo Dicaprio released? What is this year raised to the 0.43 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhQkOE6YCM0p"
   },
   "source": [
    "##  Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "_jDIYjfEB-FU"
   },
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "kdxc6fhTCUVS"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8k9uNQ1hCXwE",
    "outputId": "bc8d0243-673d-4278-ca34-c2367ecbc024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Sabroso Cantina\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gwB6mlGrE74a",
    "outputId": "29f62b03-323d-4ef9-9983-59ace21c2502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Maharaja's Palace: A Taste of India\"\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Indian\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5zZb8oPHE7v9",
    "outputId": "4fe355bc-d576-4bc3-b530-292eb452c6ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TA_Z6bZPDbue"
   },
   "source": [
    "## ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZyTmlDRZDcWu",
    "outputId": "91f38a1f-68f9-4114-9397-757e078a0dae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "rGBeWBB1Ddhf",
    "outputId": "9cff2e13-2cf3-40fd-865f-6f7cec594cf1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' The first cricket world cup was held in 1975 and was won by the West Indies team. It was a 60-over match between the West Indies and Australia, with the West Indies scoring 291 runs and Australia scoring 274 runs. The West Indies won by 17 runs.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "4sMP3AAYDdd3",
    "outputId": "d1842544-dabd-4b86-cbdb-f60b1505917b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'  5+5 is equal to 10. This is because the basic mathematical operation of addition is used to combine two numbers and find their total sum. In this case, 5 and 5 are added together to equal 10. This is a basic concept taught in elementary mathematics.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "w1QtCNXGDdaL",
    "outputId": "90eb7743-9f0f-47c4-a27c-e35f9b6a904f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" The captain of the West Indies team in the first cricket world cup was Clive Lloyd. He was a left-handed batsman and a medium-pace bowler. He also played in the finals and scored 102 runs, contributing to the team's victory.\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwI4m9YuC_ut"
   },
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQ_XVFYPFPLm",
    "outputId": "29ebeb2b-5e49-4115-ae2f-5341d2122b3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"El Sabroso Cantina\" \n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
    "name = chain.run(\"Mexican\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N7VVo8APFPIm",
    "outputId": "15e87096-f35b-401f-ccf2-769744b3a709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"Taj Mahal Bites\"\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Indian\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eLrEpJiJFPFk",
    "outputId": "6605c49c-ecc0-468d-cc84-397ba3409d51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Mexican\n",
      "AI: \n",
      "\"El Sabroso Cantina\" \n",
      "Human: Indian\n",
      "AI: \n",
      "\"Taj Mahal Bites\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGtiHeZSFPB8",
    "outputId": "4e00e9f2-7c28-45af-c00a-721d741a95be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Royal Bengal Bites\"\n"
     ]
    }
   ],
   "source": [
    "name = chain.run(\"Bengali\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1Py_qpQFiMV"
   },
   "source": [
    "## ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvpfBLZSFO1d",
    "outputId": "2ee7ee29-c541-4f1c-8449-8c6acf7c7d55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "E4V6APm4Fn8n",
    "outputId": "94830a5d-c176-4d0c-d055-992529e3ee69"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" The first cricket world cup was won by the West Indies in 1975. The final match was played between the West Indies and Australia at Lord's Cricket Ground in London. The West Indies won by 17 runs. The West Indies team was led by Clive Lloyd and had notable players such as Viv Richards, Gordon Greenidge, and Andy Roberts. The tournament was organized by the International Cricket Council and featured eight teams from around the world. The first match was played between England and India at Lord's on June 7, 1975. India won by 202 runs. \""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "4qXKsHIJFn6q",
    "outputId": "7e473539-fdef-4d3a-ae8f-86f90b3d94f2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' 5+5 is equal to 10.'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dFx44YvmFn3v",
    "outputId": "0bfeb998-c7f5-48cd-be38-4fc231314e88"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' The captain of the winning team, the West Indies, was Clive Lloyd.'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N81zXqZ7F04B"
   },
   "source": [
    "#ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "nOIvZIqCFnvZ",
    "outputId": "61f0cfa5-0ba5-4759-95a4-86bf3a7c25ef"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' The first cricket world cup was won by the West Indies in 1975. The tournament was held in England and the West Indies defeated Australia in the final to become the first ever world champions. The West Indies team was led by Clive Lloyd, who was also named as the player of the tournament for his outstanding performance throughout the tournament. The inaugural cricket world cup was a major success and paved the way for future tournaments and the growth of the sport globally.'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "convo = ConversationChain(\n",
    "    llm=OpenAI(temperature=0.7),\n",
    "    memory=memory\n",
    ")\n",
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "a7j2k9N1F8AS",
    "outputId": "b67416e5-7eff-4043-cb80-8ba99c321888"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'  Five plus five is equal to ten. This is a basic mathematical equation that is commonly used in everyday life. The result of five plus five is also known as the sum and is represented by the symbol \"+\". It is an important concept in mathematics and is used in various calculations and equations. Did you know that the concept of addition has been around since ancient times and was used by civilizations such as the Egyptians and Babylonians?'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "IdfazTD5F7-A",
    "outputId": "ae8dfc1a-f99a-4e9b-c812-9aee097bae8d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' The captain of the winning team, the West Indies, was Clive Lloyd. He was a highly skilled and respected cricketer who played for the West Indies from 1966 to 1985. Lloyd was known for his strong leadership skills and aggressive batting style, making him a key player in the West Indies team. He was also the captain of the team during their successful back-to-back World Cup wins in 1975 and 1979. Lloyd is considered one of the greatest cricketers of all time and was inducted into the ICC Hall of Fame in 2009.'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jova770gGCc2"
   },
   "source": [
    "## Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ToZKBeVdF7yo",
    "outputId": "f26bac0d-3017-4bb5-dcec-76effff18db4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "S8ne5xmsF7vv"
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"/content/ChatGPT Prompt Engineering 100323.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDitDdNRF7ou",
    "outputId": "62becb90-cd48-4b7f-c378-f48497afce94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='ChatGPT Prompt Engineering GuideGenerative AI Overview for Project Managers', metadata={'source': '/content/ChatGPT Prompt Engineering 100323.pdf', 'page': 0}),\n",
       " Document(page_content='Tailoring Project Management Prompts for ChatGPT \\nWhen tailoring prompts for ChatGPT or other AI chatbots, make \\nyour instructions as clear and specific as possible. \\nUse short, clear, concise sentences. \\nLimit the scope.\\nRevise your questions based on the response you receive.\\nEXAMPLE FORMAT:\\nAct as a {role}. {Question} with {action} using a {tone}.\\nEXAMPLE:\\nAct a senior IT project manager for every question that follows \\nand answer with quantifiable results using a personal tone. \\n2 Generative AI Overview for Project Managers | Resources', metadata={'source': '/content/ChatGPT Prompt Engineering 100323.pdf', 'page': 1}),\n",
       " Document(page_content='Most Useful Output Formats\\nChatGPT can generate data in the following formats:\\nPlain Text\\nASCII\\nHTML\\nJavaScript\\nCSV\\nTablePlain Text files store human-readable data without \\nformatting or stying, using simple text editors\\nASCII is a standard code used by computers to represent \\nletters, numbers, and symbols for sharing text\\nHTML is the standard language for creating web pages \\nwith tags for defining structure, layout, and styling\\nJavaScript is a programming language used to create \\ndynamic and interactive content on web pages\\nCSV stores tabular data as comma-separated values in a \\nsimple text-based format\\nA table is used to organize information in rows and \\ncolumns, making it easy to read and understand\\n3 Generative AI Overview for Project Managers | Resources', metadata={'source': '/content/ChatGPT Prompt Engineering 100323.pdf', 'page': 2}),\n",
       " Document(page_content='Make a Decision\\nPROMPT :\\nI am working with a software \\ndevelopment team facing a decision: \\noutsource or hire an in-house designer. \\nOutsourcing would cost $20K up front \\nand $5K per month for 6 months. Our \\nin-house designer would cost $60K in \\nsalary. Show me the total cost for each \\nusing a detailed cost-benefit analysis in \\na table format. Explain your suggestion. \\n4 Generative AI Overview for Project Managers | Resources', metadata={'source': '/content/ChatGPT Prompt Engineering 100323.pdf', 'page': 3}),\n",
       " Document(page_content='Create a Persuasive \\nBusiness Case\\nPROMPT :\\nTrinidad Foods wants to introduce a new \\nloyalty program to increase customer \\nretention and sales. The program will \\ncost $50K to implement but is projected \\nto increase revenue by 10% in the first \\nyear. This means the store will generate \\nan additional $100K in revenue, yielding a \\nnet profit of $50K. The ROI for the loyalty \\nprogram investment is 100%.\\nCompose a business case to the decision \\nmaker that is clear and concise and uses a \\npersuasive and personal tone. Highlight in \\nbold any important information.\\n5 Generative AI Overview for Project Managers | Resources', metadata={'source': '/content/ChatGPT Prompt Engineering 100323.pdf', 'page': 4}),\n",
       " Document(page_content='Create a \\nProject Charter\\nPROMPT :\\nA 10,000 square meter commercial building \\nin Accra, Ghana is proposed to meet the \\ngrowing demand for commercial space. \\nIt must comply with building codes, meet \\nmodern business needs, and stay within \\na $20M budget and 24-month timeline. \\nCreate a project charter and include plans \\nfor design and construction, procurement, \\nquality checks, and building code \\ncompliance. The project manager leads 20 \\nteam members. Key stakeholders include \\nthe client, government authorities, and the \\ngeneral public. \\n6 Generative AI Overview for Project Managers | Resources', metadata={'source': '/content/ChatGPT Prompt Engineering 100323.pdf', 'page': 5}),\n",
       " Document(page_content='Manage Scope Creep\\nPROMPT :\\nI am a project manager developing a new \\nmobile app with an original scope of 10 tablets \\nfor users. However, the is client adding new \\nrequirements, including five new tablets, two \\nuser roles and three features which include \\nsingle sign-on, Microsoft Azure cloud security, \\nand ChatGPT bot integration. The client insists \\nthe app must meet all these requirements. The \\nteam estimated the additions will extend the \\ntimeline by 3 months and cost $50K; the original \\nbudget was $60K. I need to manage the scope \\ncreep and communicate with the client about \\nthe impact on the project timeline and budget. \\nCreate a requirements traceability matrix in \\na table format. Help me manage the scope \\ncreep and communicate with the client about \\nthe impact on the project timeline and budget. \\nHighlight the suggested solution(s).\\n7 Generative AI Overview for Project Managers | Resources', metadata={'source': '/content/ChatGPT Prompt Engineering 100323.pdf', 'page': 6}),\n",
       " Document(page_content='Calculate Earned Value \\nManagement (EVM)\\nPROMPT :\\nA software development project has a \\n$100K budget and a timeline of 6 months. \\nAfter 3 months, the actual cost (AC) is $60K \\nand the project is 50% complete.\\nAm I over or under budget? Behind or \\nahead of schedule? What is the Earned \\nValue (EV), Estimate to Complete (ETC), and \\nVariance at Completion (VAC)? Show this in \\na table format and highlight the explanation \\nin bold.\\n8 Generative AI Overview for Project Managers | Resources', metadata={'source': '/content/ChatGPT Prompt Engineering 100323.pdf', 'page': 7}),\n",
       " Document(page_content='Create a \\nPower/Interest Grid\\nPROMPT:\\nAfter conducting a stakeholder analysis for \\nour project to build a new solar farm, it is \\nobvious that our Energy Minister, Ken, and \\nproject sponsor, Kevin, should be managed \\nclosely to ensure their needs are met. Our \\nproject coordinator, Carol, does not seem \\nto have much influence on the project, but \\nshe does have a keen interest in it. Our \\nvendor, Alyssa, does not have any influence, \\nbut she has a high interest in the project.\\nShow me, in detail, a power/interest grid in \\na table format based on “keep satisfied,” \\n“manage closely,” “monitor,” and “keep \\ninformed.”\\n9 Generative AI Overview for Project Managers | Resources', metadata={'source': '/content/ChatGPT Prompt Engineering 100323.pdf', 'page': 8}),\n",
       " Document(page_content='Identify the Critical Path\\nPROMPT:\\nI am a project manager for a construction \\ndevelopment project. The project has five activities: \\n• Activity 1 {predecessor = start and duration = 3 days}\\n• Activity 2 {predecessor = Activity 1 and duration = 3 days}\\n• Activity 3 {predecessor = Activity 1 and duration = 4 days}\\n• Activity 4 {predecessor = Activity 2 and duration = 8 days}\\n• Activity 5 {predecessor = Activity 3, 4 and duration = 4 days}\\nShow me, in a table format, the dependencies \\nbetween the tasks and their corresponding early \\nstart (ES), early finish (EF), late start (LS), late finish \\n(LF) times; float for each task; and all the paths with \\nduration. Solve this and show your work concisely \\nwith the critical path and all other paths in days. \\nHighlight the critical path with the shortest duration \\nand least float value of 0 in bold. \\n10 Generative AI Overview for Project Managers | Resources', metadata={'source': '/content/ChatGPT Prompt Engineering 100323.pdf', 'page': 9}),\n",
       " Document(page_content='Create a \\nProject Schedule\\n(Continued from Critical Path data) \\nPROMPT:\\nWith the data from the previous critical \\npath exercise, create a detailed project \\nschedule with placeholder activity \\nnames.\\n11 Generative AI Overview for Project Managers | Resources', metadata={'source': '/content/ChatGPT Prompt Engineering 100323.pdf', 'page': 10}),\n",
       " Document(page_content='Create a Cause and \\nEffect Diagram\\nPROMPT:\\nI am a construction project manager \\nexperiencing a delay in completion of a \\nproject. I want to identify the root cause(s) \\nof the problem. In a diagram, categorize the \\npossible causes into equipment, material, \\npersonnel, and weather-related issues. \\nThe quantifiable numeric data should show \\nthat material delays are the primary cause \\nof the delay, accounting for 70% of the \\ntotal project delay, followed by personnel \\nshortages at 20% and weather-related \\nissues at 10%.\\nShow me how to solve this using a horizontal \\ndetailed cause and effect diagram in ASCII \\nand identify ways to mitigate the delay. \\nHighlight the probable root cause in bold.\\n12 Generative AI Overview for Project Managers | Resources', metadata={'source': '/content/ChatGPT Prompt Engineering 100323.pdf', 'page': 11}),\n",
       " Document(page_content='Create User Stories\\nPROMPT :\\nA company requires an airline mobile \\napp to allow frequent flyers to view and \\nmanage their upcoming flights. The app \\nshould enable users to make flight changes \\neasily and stay organized. Success will be \\nmeasured by the number of successful \\nlogins to the app, the number of flight \\nchanges made, and overall user satisfaction \\nwith the new feature. This data will inform \\nfuture improvements to the app’s flight \\nmanagement functionality. \\nCreate an agile user story in two tables. \\nUse the following as a guideline: for the user \\nstory, create a sentence using the phrases \\n“As a [persona],” “I want [what],” and “so that \\n[desired outcome] ”. The acceptance criteria \\nuse the Given-When-Then formula.\\n13 Generative AI Overview for Project Managers | Resources', metadata={'source': '/content/ChatGPT Prompt Engineering 100323.pdf', 'page': 12}),\n",
       " Document(page_content='Manage Risk\\nPROMPT :\\nThe Fukushima Daiichi nuclear disaster \\nin 2011 demonstrated poor risk \\nmanagement. Despite a 10% chance \\nof a tsunami exceeding the planned \\nmaximum of 5.7 meters, TEPCO did not \\nrevise safety measures. The resulting \\n14-meter tsunami caused a level-7 \\nnuclear disaster, costing over US$200 \\nbillion in damages and cleanup.\\nShow me how to solve this concisely \\nusing a detailed risk analysis table \\nformat. Highlight the suggested solution \\nin bold.\\n14 Generative AI Overview for Project Managers | Resources', metadata={'source': '/content/ChatGPT Prompt Engineering 100323.pdf', 'page': 13}),\n",
       " Document(page_content='Other Project Deliverables with ChatGPT\\nProblem-solving \\ntechniques\\nInnovation & \\nIdea Generation\\nCompetetive \\nAnalysis\\nProject \\nSummarization\\nProject \\nDecision-making\\nBusiness\\nCase\\nProject \\nCharter\\nSOW\\nRFP, RFI, RFQBriefing\\nNote\\nResource Allocation\\nPlan\\nStakeholder \\nManagement Plan\\nProject \\nManagement Plan\\nProcurement and Contact \\nManagement\\nBudget and \\nFinancial\\nForecasting\\nCommunication \\nPlan\\nChange \\nManagementKnowledge\\nManagement\\nQuality\\nAssurance\\nFeasibility\\nTraining\\nPlan\\nProject\\nClosure\\nDisaster Recovery/\\nContingency Plan\\nVendor\\nManagement\\nProject Audit\\n& Guidelines\\nStatus & \\nProgressRequirements \\nTraceability Matrix\\nEarned Value\\nManagement\\nPower/Interest\\nGrid\\nCritical\\nPath\\nProject\\nSchedule\\nCause and Effect\\nDiagram\\nUser\\nStories\\nAgile\\nPersonas\\nRisk\\nAnalysisMeeting Minutes &\\nAction Items\\nTask\\nPrioritization\\nChange\\nRequest\\nAcceptance\\nForm\\nRegisters\\nPost\\nImplementation\\nDeployment\\n15 Generative AI Overview for Project Managers | Resources', metadata={'source': '/content/ChatGPT Prompt Engineering 100323.pdf', 'page': 14})]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "V6Sj1IIjC8WI",
    "outputId": "22186cb3-da3d-44ee-af70-df295098e3b0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" The first cricket world cup was won by the West Indies team in 1975. They defeated Australia in the final match by 17 runs. The tournament was held in England and was organized by the International Cricket Council. The West Indies team was led by Clive Lloyd and included legendary players such as Viv Richards and Andy Roberts. This victory marked the beginning of West Indies' dominance in the cricket world, as they went on to win the next two consecutive world cups in 1979 and 1983.\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=3)\n",
    "\n",
    "convo = ConversationChain(\n",
    "    llm=OpenAI(temperature=0.7),\n",
    "    memory=memory\n",
    ")\n",
    "convo.run(\"Who won the first cricket world cup?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "AcVBawLuC-nq",
    "outputId": "7a65e9f8-28d7-4725-a428-74c98704bd00"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' The answer to 5+5 is 10.'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "lnqRt1nbDE90",
    "outputId": "436897c7-c376-4118-c75e-8b9decbe3e4d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' The answer to 5+5 is 10.'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"How much is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "gIfFUn26DHX2",
    "outputId": "b31acb54-4574-4a9c-b1ca-c86b51934912"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' The captain of the winning team was Clive Lloyd of the West Indies team. He was a well-respected and successful captain, leading the team to multiple victories in the cricket world cup. He was known for his strong leadership skills and strategic thinking on the field. After his retirement from cricket, he also served as the chairman of the West Indies cricket board.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PjaASFKEA0s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
